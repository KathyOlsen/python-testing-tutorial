{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Testing Tutorial\n",
    "\n",
    "Welcome to the Python Testing Tutorial notebook. This will be the resource we use to both learn, and test what we learned. This notebook is designed to be loaded to a Jupyter Lab instance that has `pytest` and `ipython_pytest` installed. In a new virtual environment, do\n",
    "\n",
    "```\n",
    "$ pip install pytest ipython_pytest jupyterlab\n",
    "```\n",
    "\n",
    "When this is done, launch Jupyter\n",
    "\n",
    "```\n",
    "$ jupyter lab\n",
    "```\n",
    "\n",
    "Click on the upload icon, and upload this notebook.\n",
    "\n",
    "The next step will be to load the `ipython_test` Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipython_pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any output from this step. If an error occured saying it is not installed, make sure the virtual environment has `ipython_test` installed.\n",
    "\n",
    "Next, we will run one test, just to see what failure looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_something():\n",
    "    assert [1] == [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your test didn't run, or you did not get a failure that looks like\n",
    "\n",
    "```\n",
    "    def test_something():\n",
    ">       assert [1] == [2]\n",
    "E       assert [1] == [2]\n",
    "E         At index 0 diff: 1 != 2\n",
    "E         Use -v to get the full diff\n",
    "\n",
    "_ipytesttmp.py:3: AssertionError\n",
    "```\n",
    "\n",
    "Then something might not be installed and configured properly. Check again that `pytest` and `ipython_pytest` are properly installed in your virtual environment.\n",
    "\n",
    "The mechanics of running tests differ between environments. Usually, you will be running the `pytest` command-line -- but quite often, not directly. You might be using a `tox` runner, run the tests in a Continuous Integration environment, or maybe inside a Docker container. Regardless, most of the effort will be spent writing tests and looking at test failures -- and this is going to be what this tutorial will cover.\n",
    "\n",
    "Pytest uses the Python keyword `assert` in order to check conditions in tests. It internally processes this statement to help failures look nicer. We already saw that it does a checks the difference with a list of one element. Let's see a few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_missing():\n",
    "    assert [1] == []\n",
    "    \n",
    "def test_extra():\n",
    "    assert [1] == [1, 2]\n",
    "    \n",
    "def test_different():\n",
    "    assert [1, 2, 3] == [1, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tests failed here -- but read the output carefully to see *how* they failed. When writing tests for new code, much of your work will be analyzing test failures to understand how they failed, and whether the test code or product code is broken. If the tests are doing their job to prevent regressions, much of your work will be looking at test failures to understand whether you need to fix the code or modify an out-of-date test.\n",
    "\n",
    "The most interesting thing about any test framework is how test failures look.\n",
    "\n",
    "Let's see how `pytest` handles other equality failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_string():\n",
    "    assert \"hello\\nworld\" == \"goodbye\\nworld\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For strings, `pytest` will do a line-by-line diff to highlight differences. However, it will not do inside-line-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_set():\n",
    "    assert set([1,2]) == set([1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sets, it will check for spurious elements on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more code we put on the assertion line, the more detailed the output will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def add(x, y):\n",
    "    return x+y\n",
    "    \n",
    "def test_add():\n",
    "    assert add(1, 2) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes equality is too strict: after all many functions we write, it's hard to know what the output will be equal to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import random\n",
    "\n",
    "def test_random():\n",
    "    assert random.uniform(0, 1) > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for set membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import random\n",
    "\n",
    "def test_choice():\n",
    "    assert random.choice([1, 2, 3]) in [4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing edge-cases of code is particularly important. Specifically, we might want to test whether the code raises the right exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_exception():\n",
    "    with pytest.raises(ValueError):\n",
    "        1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now it is time to fix some tests. In each cell, only change the line that says \"fix this line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def add(a, b):\n",
    "    return a # fix this line\n",
    "\n",
    "def test_add():\n",
    "    asserrt add(1, 2) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def append(a, b):\n",
    "    pass # fix this line\n",
    "\n",
    "def test_add():\n",
    "    a = [1, 2, 3]\n",
    "    append(a, 4)\n",
    "    assert a == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def safe_remove(a, b):\n",
    "    pass # fix this line\n",
    "\n",
    "def test_safe_remove_no():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 3)\n",
    "    assert 1 in things\n",
    "\n",
    "def test_safe_remove_yes():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 2)\n",
    "    assert 2 not in things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def get_min_max(a, b):\n",
    "    return a, b # fix this line\n",
    "\n",
    "def test_min_max_high():\n",
    "    a, b = get_min_max(2, 1)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b\n",
    "\n",
    "def test_min_max_low():\n",
    "    a, b = get_min_max(1, 2)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_no_element():\n",
    "    thing = {} # fix this line\n",
    "    with pytest.raises(IndexError):\n",
    "        thing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "* Tests check whether things are true\n",
    "* pytest uses `assert`\n",
    "* Put calculations on the `assert` line to get more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking\n",
    "\n",
    "Sometimes, using real objects is hard, ill-advised, or complicated.\n",
    "\n",
    "For example, a `requests.Session` connects to real websites:\n",
    "using it in your unittests invites a...lot...of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mocks\" are a unittest concept: they produce objects that are substitutes for the real ones.\n",
    "There's a whole cottage industry that will explain that \"mock\", \"fake\", and \"stub\" are all subtly different.\n",
    "We'll use all of those interchangably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = mock.Mock()\n",
    "\n",
    "def do_something(o):\n",
    "    return o.something(5)\n",
    "\n",
    "do_something(regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks have \"all the methods\". The methods will usually return another Mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words from a pathlib.Path\n",
    "import contextlib\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    with contextlib.closing(path_obj.open()) as fpin:\n",
    "        result = fpin.read()\n",
    "    return result.split()\n",
    "\n",
    "words_from_path(regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want it to return a \"real\" string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import contextlib\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    with contextlib.closing(path_obj.open()) as fpin:\n",
    "        result = fpin.read()\n",
    "    return result.split()\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "my_path = mock.Mock()\n",
    "my_path.open.return_value.read.return_value = \"\"\"\\\n",
    "In winter, when the fields are white,\n",
    "I sing this song for your delight.\n",
    "In spring, when woods are getting green,\n",
    "I'll try and tell you what I mean.\n",
    "In summer, when the days are long,\n",
    "Perhaps you'll understand the song.\n",
    "In autumn, when the leaves are brown,\n",
    "Take pen and ink, and write it down.\n",
    "\"\"\"\n",
    "\n",
    "def test_jabberwocky():\n",
    "    result = words_from_path(my_path)\n",
    "    assert \"winter\" in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
